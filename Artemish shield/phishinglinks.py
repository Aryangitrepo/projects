# -*- coding: utf-8 -*-
"""phishinglinks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ci3qnWbfxSu4BZGzBGpzltcXWpjIUhOB
"""

# Data Processing
import pandas as pd
import numpy as np

# Modelling
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint
from sklearn.preprocessing import LabelEncoder

# Tree Visualisation
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

df = pd.read_csv('/content/dataset_phishing.csv')

df.head()

df.describe()

df.info()

X = df.drop(['url', 'status'], axis=1) #we are dropping url as its just a string value
y = df['status'] #setting status as the target variable and using rest for prediction

#removing -1
#first setting it to null and then replacing with median value for binninb technique
for col in ['domain_registration_length', 'domain_age', 'web_traffic']:
    if col in X.columns:
        X[col] = X[col].replace(-1, np.nan)
        X[col] = X[col].fillna(X[col].median())

le = LabelEncoder() #we  are encoding the target variable to 0 /1
y_encoded = le.fit_transform(y) #legitimate will be 0 and phishing will be 1

print("Data preparation complete. X shape:", X.shape, "y_encoded shape:", y_encoded.shape)
print("First 5 rows of preprocessed X:")
print(X.head().to_markdown(index=False, numalign="left", stralign="left"))
print("\nFirst 5 values of encoded y:", y_encoded[:5])

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42) #splitting for training and testing the data 20% for testing 80% training

print("\nData splitting complete:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores

# Train the model using the training data
print("\nTraining Random Forest model...")
rf_classifier.fit(X_train, y_train)
print("Model training complete.")

from sklearn.metrics import classification_report
# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy: {accuracy:.4f}")

# Display classification report (precision, recall, f1-score)
# 'target_names' map back to original labels if needed, based on LabelEncoder's fit
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# Display confusion matrix
# It shows correct vs. incorrect predictions for each class
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nInterpretation of Confusion Matrix:")
print(f"True Negatives (Legitimate classified as Legitimate): {conf_matrix[0, 0]}")
print(f"False Positives (Legitimate classified as Phishing): {conf_matrix[0, 1]}")
print(f"False Negatives (Phishing classified as Legitimate): {conf_matrix[1, 0]}")
print(f"True Positives (Phishing classified as Phishing): {conf_matrix[1, 1]}")

from sklearn.model_selection import train_test_split, GridSearchCV
#settings 1 trying out hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}


grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                           param_grid=param_grid,
                           cv=3,
                           n_jobs=-1,
                           verbose=2)
print("\nPerforming GridSearchCV for hyperparameter tuning (this may take a while)...")
grid_search.fit(X_train, y_train)

print("\nBest parameters found:", grid_search.best_params_)
print("Best score found (cross-validation accuracy):", grid_search.best_score_)

best_rf_model = grid_search.best_estimator_
y_pred_tuned = best_rf_model.predict(X_test)
print("\nAccuracy with tuned model on test set:", accuracy_score(y_test, y_pred_tuned))

from sklearn.metrics import classification_report
print("\nClassification Report with tuned model on test set:")

target_names = le.inverse_transform([0, 1])
print(classification_report(y_test, y_pred_tuned, target_names=target_names))

import joblib
joblib.dump(rf_classifier, 'phishing_detector_model.pkl')
# To load: loaded_model = joblib.load('phishing_detector_model.pkl')

import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import LabelEncoder
from urllib.parse import urlparse
import tldextract
import re

try:
    rf_classifier = joblib.load('phishing_detector_model.pkl')
    le = LabelEncoder()
    label_mapping = {0: 'legitimate', 1: 'phishing'}

except FileNotFoundError:
    print("Error: Model or LabelEncoder not found. Please ensure 'phishing_detector_model.pkl' exists.")
    print("You might also need to re-initialize and fit LabelEncoder if not saved.")
    exit()

def extract_url_features(url):
    """Extract features from a URL for phishing detection"""

    # Initialize features with default values
    features = {
        'url_length': len(url),
        'num_dots': url.count('.'),
        'num_hyphens': url.count('-'),
        'num_underscore': url.count('_'),
        'num_slash': url.count('/'),
        'num_questionmark': url.count('?'),
        'num_equal': url.count('='),
        'num_at': url.count('@'),
        'num_and': url.count('&'),
        'num_exclamation': url.count('!'),
        'num_space': url.count(' '),
        'num_tilde': url.count('~'),
        'num_comma': url.count(','),
        'num_plus': url.count('+'),
        'num_asterisk': url.count('*'),
        'num_hash': url.count('#'),
        'num_dollar': url.count('$'),
        'num_percent': url.count('%'),
        'domain_length': 0,
        'num_dots_in_domain': 0,
        'num_hyphens_in_domain': 0,
        'num_underscore_in_domain': 0,
        'has_https': 1 if url.startswith('https://') else 0,
        'has_http': 1 if url.startswith('http://') else 0,
        'has_ip': 1 if re.match(r'\d+\.\d+\.\d+\.\d+', url) else 0,
    }

    # Extract domain information
    try:
        ext = tldextract.extract(url)
        domain = ext.domain + '.' + ext.suffix
        features['domain_length'] = len(domain)
        features['num_dots_in_domain'] = domain.count('.')
        features['num_hyphens_in_domain'] = domain.count('-')
        features['num_underscore_in_domain'] = domain.count('_')
    except:
        pass

    # Additional features (simplified for this example)
    # In a real implementation, you would calculate all 85 features

    return features

def prepare_features_for_model(url_features):
    """Convert extracted features into the format expected by the model"""

    # This is a simplified version - you would need to map all 85 features
    # Here we're just using the example features from your original code
    # You should replace this with your actual feature extraction logic

    # Example features from your original code
    new_link_features = [
        37, 19, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0.0, 0.0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 4, 4, 3, 3, 3, 11, 11,
        6, 5.75, 7.0, 4.5, 0, 0, 0, 0, 0, 0, 17, 0.5294117647058824, 0.47058823529411764,
        0, 0, 0, 0.875, 0, 0.5, 0, 0, 80.0, 0, 100.0, 0.0, 0, 0, 0, 0.0, 0, 0, 0,
        0, 1, 45, -1, 0, 1, 1, 4
    ]

    # Replace with actual extracted features where possible
    new_link_features[0] = url_features['url_length']
    new_link_features[1] = url_features['num_dots']
    new_link_features[3] = url_features['num_hyphens']
    new_link_features[22] = url_features['has_https']
    new_link_features[25] = url_features['has_http']
    new_link_features[26] = url_features['has_ip']

    median_domain_registration_length = 208
    median_domain_age = 589
    median_web_traffic = 0

    if new_link_features[82] == -1:
        new_link_features[82] = median_domain_registration_length
    if new_link_features[83] == -1:
        new_link_features[83] = median_domain_age
    if new_link_features[84] == -1:
        new_link_features[84] = median_web_traffic

    return np.array(new_link_features).reshape(1, -1)

def main():
    print("Phishing URL Detector")
    print("---------------------\n")

    while True:
        url = input("Enter the URL to check (or 'q' to quit): ").strip()

        if url.lower() == 'q':
            break

        if not url:
            print("Please enter a valid URL.")
            continue

        # Extract features from the URL
        try:
            url_features = extract_url_features(url)
            features_array = prepare_features_for_model(url_features)

            # Make prediction
            prediction_encoded = rf_classifier.predict(features_array)
            predicted_label_text = label_mapping.get(prediction_encoded[0], "Unknown")

            print(f"\nThe link is classified as: {predicted_label_text}\n")
        except Exception as e:
            print(f"Error processing URL: {str(e)}")

if __name__ == "__main__":
    main()

!pip install tldextract

